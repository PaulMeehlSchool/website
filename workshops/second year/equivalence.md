---
title:  "Equivalence Testing and the Smallest Effect Size of Interest"
mathjax: false
layout: post
categories: media
---

## Course Description

When testing hypotheses researchers should be able to corroborate their prediction, as well as falsify it. Most researchers have only been taught null-hypothesis significance testing, and therefore do not have the statistical tools to provide evidence for the absence of an effect. Equivalence tests have been developed specifically to allow researchers to conclude that the effect they have observed, if any, is too small to be considered support for their prediction. Learning about equivalence tests is important to be able to publish informative ‘null effects’, design studies that yield informative results about the presence or absence of effects, and prevents common misinterpretations of p-values. 
In this workshop we will explain the underlying rationale of interval hypothesis tests such as equivalence tests, inferiority tests, superiority tests, and minimal effect tests, and how they complement null hypothesis testing. In a hands-on session we will illustrate how to perform equivalence tests in different open source statistical software packages such as R, JASP and jamovi. We will also discuss different approaches to specifying a smallest effect size of interest to test against in an equivalence test. Finally, we will discuss more advanced aspects of interval hypothesis tests, such as a-priori power analyses and three-sided hypothesis tests. 

<br>

## Prerequisites 

A basic understanding of null hypothesis testing is required for this workshop. 

<br>

## Reading Materials

### Required

Equivalence Testing and Interval Hypotheses: [https://lakens.github.io/statistical_inferences/09-equivalencetest.html](https://lakens.github.io/statistical_inferences/09-equivalencetest.html) 

<br>

### Capacity

This course has a maximum capacity of 40 participants.

<br>

### Time and Location

This workshop will be held <ins>**on-site only**</ins> at Eindhoven University of Technologyon **December 12, 2024**. Details will be provided to all attendees over email after registration for the workshop.

Workshops start from 9:30 to 16:30 with a lunch break from 12:30 to 13:30. Lunch will not be provided but can be purchased at the university canteen or the on-campus supermarket. 

<br>

### Registration

To register for this workshop, please complete the following form by **Nov 25th**. Note that your registration will be considered finalized only after receiving a confirmation email.

[Registration Form](https://forms.office.com/Pages/ResponsePage.aspx?id=R_J9zM5gD0qddXBM9g78ZP_Kihp-VglPgWom9gajHXdURDJHTFU4U1k1NDlTNTEyUEtCQUJYRFRGVS4u)

<br>

### Instructors

**Dr. Daniel Lakens**

Daniel Lakens is an Associate Professor of Metascience and chair of the Ethical Review Board at the Human-Technology Interaction group at Eindhoven University of Technology in The Netherlands. Lakens’ work focuses on improving research methods and statistical inferences in the social sciences. He has published more than 100 peer-reviewed articles, including highly cited papers on effect sizes, sequential analyses, equivalence testing, and sample size justification. He won the Ammodo Science award for fundamental research in the Social Sciences in 2023. 

**Dr. Paul Riesthuis**

Paul Riesthuis is a postdoctoral researcher at the Faculty of Law and Criminology at KU Leuven. His postdoctoral research (funded by FWO) focuses on how to improve the practical and statistical inferences of eyewitness memory research using the smallest effect size of interest. Additionally, he conducts research in the field of eyewitness memory, fake news, and the illusory truth effect. 

**Jack Fitzgerald**

Jack Fitzgerald is a PhD candidate in economics at Vrije Universiteit Amsterdam. His work focuses on applied econometrics, replication, and the economics of science. He has written several papers that both apply and develop equivalence testing methods. These include large-scale replication projects that use equivalence testing to show that false negative rates for null findings in the published literature are unacceptably high. 
